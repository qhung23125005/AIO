{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first exercise, we will deal with **Part-of-Speech (POS) Tagging** problem. We will fine-tune model from Hugging face and fine tune it using Penn Tree Bank dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the corpus: 3914\n",
      "First sentence with tags: [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "print(\"Number of sentences in the corpus:\", len(tagged_sentences))\n",
    "print(\"First sentence with tags:\", tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: ['pierre', 'vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.']\n",
      "First sentence tags: ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences, sentence_tags = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    # Convert tags to BIO format\n",
    "    sentences.append([word.lower() for word in sentence])\n",
    "    sentence_tags.append([tag for tag in tags])\n",
    "\n",
    "# Print the first sentence and its tags\n",
    "print(\"First sentence:\", sentences[0])\n",
    "print(\"First sentence tags:\", sentence_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort them (optional, for consistency)\n",
    "label2id = {tag: i for i, tag in enumerate(set(tag for tags in sentence_tags for tag in tags))}\n",
    "id2label = {i: tag for tag, i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(\n",
    "    sentences, sentence_tags, test_size=0.3\n",
    ")\n",
    "valid_sentences, test_sentences, valid_tags, test_tags = train_test_split(\n",
    "    test_sentences, test_tags, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "class PosTaggingDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                sentences: List[List[str]], \n",
    "                tags: List[List[str]], \n",
    "                tokenizer,\n",
    "                label2id,\n",
    "                max_length = MAX_LENGTH):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        word_tags = self.tags[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Align labels with tokenized inputs\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Map token position to word index\n",
    "        labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                labels.append(-100)  # Ignored by loss\n",
    "            else:\n",
    "                labels.append(self.label2id[word_tags[word_id]])\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PosTaggingDataset(\n",
    "    sentences=train_sentences,\n",
    "    tags=train_tags,\n",
    "    tokenizer=tokenizer,\n",
    "    label2id=label2id,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "valid_dataset = PosTaggingDataset(\n",
    "    sentences=valid_sentences,\n",
    "    tags=valid_tags,\n",
    "    tokenizer=tokenizer,\n",
    "    label2id=label2id,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "test_dataset = PosTaggingDataset(\n",
    "    sentences=test_sentences,\n",
    "    tags=test_tags,\n",
    "    tokenizer=tokenizer,\n",
    "    label2id=label2id,\n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "\n",
    "ignore_label = -100\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions , labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "    }  \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20924\\1436446183.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1720/1720 07:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.137313</td>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.100538</td>\n",
       "      <td>0.973013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.093588</td>\n",
       "      <td>0.976101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.095172</td>\n",
       "      <td>0.976624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.093030</td>\n",
       "      <td>0.978192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.097347</td>\n",
       "      <td>0.977954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.101423</td>\n",
       "      <td>0.977622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.103546</td>\n",
       "      <td>0.977907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.105050</td>\n",
       "      <td>0.978239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.105121</td>\n",
       "      <td>0.977954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1720, training_loss=0.20879248020260832, metrics={'train_runtime': 467.7171, 'train_samples_per_second': 58.561, 'train_steps_per_second': 3.677, 'total_flos': 3579882599208960.0, 'train_loss': 0.20879248020260832, 'epoch': 10.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable wandb logging\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: the proposal comes as a surprise even to administration officials and temporarily throws into chaos the house's work on clean - air legislation.\n",
      "Predicted tags: NN NN VBZ IN DT NN RB TO NN NNS CC RB `` IN NN DT `` NN IN JJ JJ JJ `` \n",
      "Test sentence: mr. stronach, founder and controlling shareholder of magna, resigned as chief executive officer last year * - 1 to seek, unsuccessfully, a seat in canada's parliament.\n",
      "Predicted tags: . `` NN CC NN `` IN `` VBD IN NN NN NN JJ NN -NONE- -NONE- -NONE- TO `` `` DT NN IN `` `` \n",
      "Test sentence: the movie ends with sound, the sound of street people talking, and there isn't anything whimsical or enviable in those rough, beaten voices.\n",
      "Predicted tags: NN NN VBZ IN `` DT NN IN NN NNS `` CC `` `` NN `` CC `` IN `` `` VBN `` \n",
      "Test sentence: - - in britain, the benchmark 11 3 \\ / 4 % bond due 2003 \\ / 2007 fell 14 \\ / 32 to 111 2 \\ / 32 * - 1 to yield 10. 19 %.\n",
      "Predicted tags: . : IN `` DT `` CD CD CD CD CD NN NN JJ CD CD CD CD VBD CD CD CD CD TO CD CD CD CD CD -NONE- -NONE- -NONE- TO VB `` CD `` \n",
      "Test sentence: imports were at $ 50. 38 billion * u *, up 19 %.\n",
      "Predicted tags: . VBD IN $ `` CD CD -NONE- -NONE- `` CD CD `` \n",
      "Test sentence: countries in the region also are beginning * - 1 to consider a framework for closer economic and political ties.\n",
      "Predicted tags: VBP IN DT NN VBP VBP VBG -NONE- -NONE- -NONE- TO VB DT NN IN JJR JJ CC JJ `` \n",
      "Test sentence: strategic objectives, not financial return, drive many of the deals,'' says * t * - 1 a venture economics spokesman.\n",
      "Predicted tags: . `` RB JJ `` NN JJ IN DT `` VBZ -NONE- -NONE- -NONE- -NONE- -NONE- DT NN NNPS `` \n",
      "Test sentence: attorneys have argued since 1985, when the law took effect * t * - 1, that they can not provide information about clients who * t * - 127 don't wish their identities to be known * - 3.\n",
      "Predicted tags: . VBP VBN IN `` WRB DT NN VBD NN -NONE- -NONE- -NONE- -NONE- `` IN PRP MD RB VB NN IN NNS VBP -NONE- -NONE- -NONE- -NONE- -NONE- `` VBP PRP$ `` TO VB VBN -NONE- -NONE- `` \n",
      "Test sentence: the st. louis company earned $ 45. 2 million * u *, or 65 cents a share, compared with $ 84. 9 million * u *, or $ 1. 24 * u * a share, a year earlier.\n",
      "Predicted tags: . `` `` NN VBD $ `` CD CD -NONE- -NONE- `` CC CD NNS DT `` VBN IN $ `` CD CD -NONE- -NONE- `` CC $ `` CD -NONE- -NONE- -NONE- DT `` DT NN `` \n",
      "Test sentence: there's no question that some of those workers and managers contracted asbestos - related diseases,'' said * t * - 1 darrell phillips, vice president of human resources for hollingsworth & vose.\n",
      "Predicted tags: . DT NN IN DT IN DT NNS CC NNS VBN JJ JJ JJ `` VBD -NONE- -NONE- -NONE- -NONE- -NONE- `` `` NN NN IN NNP NNPS IN `` CC `` \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    test_sentence = test_dataset[i]['input_ids']\n",
    "    test_sentence = tokenizer.decode(test_sentence, skip_special_tokens=True)\n",
    "    print(\"Test sentence:\", test_sentence)\n",
    "    test_sentence = test_sentence.lower()\n",
    "    test_tokens =  torch.as_tensor ([ tokenizer . convert_tokens_to_ids ( test_sentence . split () ) ])\n",
    "    test_tokens = test_tokens . to ( 'cuda' )\n",
    "    outputs = model(test_tokens)\n",
    "    _ , preds = torch . max( outputs . logits , -1)\n",
    "    preds = preds [0]. cpu () . numpy ()\n",
    "    pred_tags = \"\"\n",
    "    for i in preds:\n",
    "        pred_tags += id2label[i] + \" \"\n",
    "    print(\"Predicted tags:\", pred_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
