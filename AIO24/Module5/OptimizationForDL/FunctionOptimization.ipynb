{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOzIb9Hk23MhmAG0LQfI3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qhung23125005/AIO/blob/main/AIO24/Module5/OptimizationForDL/FunctionOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization in Deep Learning\n",
        "\n",
        "Optimization is a critical component of deep learning, as it directly influences how well and how quickly a model learns from data. The primary objective of optimization algorithms is to adjust the model's parameters (such as weights and biases) to minimize a loss function that quantifies the error between the model's predictions and the true data labels. Here, we will explore:\n",
        "\n",
        "- Gradient Descent\n",
        "- Gradient Descent with Momentum\n",
        "- RMSProp\n",
        "- Adam\n",
        "\n",
        "We will try to implement those optimization techniques and optimize the function:\n",
        "\n",
        "$$\n",
        "f(w_1, w_2) = 0.1w_1^2 + 2w_2^2\n",
        "$$"
      ],
      "metadata": {
        "id": "Xiz-kpLRy3_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimize $f(w_1$$,w_2)$"
      ],
      "metadata": {
        "id": "WqqctKu7DxZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "---ZqyA928ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omiTOovVy3ao"
      },
      "outputs": [],
      "source": [
        "def df_w(W):\n",
        "  \"\"\"\n",
        "  Calculates the gradients of the function f(w1, w2)\n",
        "  Argument:\n",
        "    W = np.array([w1, w2])\n",
        "  Returns:\n",
        "    dW = np.array([df_dw1, df_dw2])\n",
        "  \"\"\"\n",
        "  df_dw1 = 0.2*W[0]\n",
        "  df_dw2 = 4*W[1]\n",
        "  dW = np.array([df_dw1, df_dw2])\n",
        "  return dW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Descent\n",
        "\n",
        "Gradient descent is an iterative optimization algorithm used to minimize a loss function by updating the model parameters in the direction opposite to the gradient. At each iteration, the parameters are adjusted to reduce the error between the model's predictions and the true values.\n",
        "\n",
        "The core update rule for gradient descent is given by:\n",
        "\n",
        "$$\n",
        "W = W - \\eta \\nabla_\\theta L(W)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $W$ represents the parameters,\n",
        "- $\\eta$ is the learning rate, controlling the step size,\n",
        "- $\\nabla_\\theta L(\\theta_t)$ is the gradient of the loss function $L$ with respect to the parameters $\\theta$ at iteration $t$.\n",
        "\n",
        "This formula shows that the parameters are updated by moving a small step in the direction that most rapidly decreases the loss, allowing the algorithm to converge towards a local (or global) minimum over time.\n"
      ],
      "metadata": {
        "id": "T_JAwDuD0-WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(W, dW, lr):\n",
        "  \"\"\"\n",
        "  Performs a single step of gradient descent\n",
        "  Argument:\n",
        "    W = np.array([w1, w2])\n",
        "    dW = np.array([df_dw1, df_dw2])\n",
        "    lr = learning rate\n",
        "  Returns:\n",
        "    W = np.array([w1, w2])\n",
        "  \"\"\"\n",
        "  W = W - lr*dW\n",
        "  return W"
      ],
      "metadata": {
        "id": "3TegfK9O2-om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sgd(w1, w2, lr, epochs):\n",
        "  #Init input\n",
        "  W = np.array([w1, w2], dtype = np.float32)\n",
        "  results = [W]\n",
        "  #Run\n",
        "  for epoch in range(epochs):\n",
        "    dW = df_w(W)\n",
        "    W = sgd(W, dW, lr)\n",
        "    results.append(W)\n",
        "  return results"
      ],
      "metadata": {
        "id": "yqXCiVqj4fJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Descent with Momentum\n",
        "\n",
        "Gradient Descent with Momentum is an enhancement of the standard gradient descent algorithm. It introduces a momentum term that helps accelerate updates in the right direction and dampen oscillations. This method accumulates a velocity vector based on past gradients, which smooths the update path and often leads to faster convergence.\n",
        "\n",
        "The update equations for Gradient Descent with Momentum are:\n",
        "\n",
        "$$\n",
        "V_t = \\beta V_{t-1} + (1 - \\beta) dW_t \\quad (2.1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_t = W_t - \\alpha \\cdot V_t \\quad (2.2)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $V_t$ is the velocity at iteration $t$,\n",
        "- $\\beta$ is the momentum coefficient (typically close to 1),\n",
        "- $dW_t$ represents the current gradient,\n",
        "- $\\alpha$ is the learning rate,\n",
        "- $W_t$ represents the model parameters at iteration $t$.\n",
        "\n",
        "These equations show that the velocity term retains part of the previous update while incorporating the new gradient, leading to faster and more stable convergence in deep learning optimization."
      ],
      "metadata": {
        "id": "Djb_0z_g57iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum(W, dW, lr, V, beta):\n",
        "  \"\"\"\n",
        "  Performs a single step of gradient descent with momentum\n",
        "  Argument:\n",
        "    W = np.array([w1, w2])\n",
        "    dW = np.array([df_dw1, df_dw2])\n",
        "    lr = learning rate\n",
        "    V = np.array([V_w1, V_w2])\n",
        "    beta = momentum coefficient\n",
        "  Returns:\n",
        "    W = np.array([w1, w2])\n",
        "    V = np.array([V_w1, V_w2])\n",
        "  \"\"\"\n",
        "  V = beta*V + (1-beta)*dW\n",
        "  W = W - lr*V\n",
        "  return W, V"
      ],
      "metadata": {
        "id": "srNYl7JV675j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_momentum(w1, w2, lr, epochs, beta):\n",
        "  #Init input\n",
        "  W = np.array([w1, w2], dtype = np.float32)\n",
        "  results = [W]\n",
        "  V = np.array([0, 0], dtype = np.float32)\n",
        "  #Run\n",
        "  for epoch in range(epochs):\n",
        "    dW = df_w(W)\n",
        "    W, V = momentum(W, dW, lr, V, beta)\n",
        "    results.append(W)\n",
        "  return results"
      ],
      "metadata": {
        "id": "HotbjK3g8JKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp Optimization\n",
        "\n",
        "RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm that helps stabilize training by scaling the learning rate for each parameter based on past gradients. It is particularly effective for handling non-stationary objectives and noisy gradients.\n",
        "\n",
        "The update equations for RMSProp are:\n",
        "\n",
        "$$\n",
        "S_t = \\gamma S_{t-1} + (1 - \\gamma) dW_t^2 \\quad (3.1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "W_t = W_t - \\alpha \\cdot \\frac{dW}{\\sqrt{S_t} + \\epsilon} \\quad (3.2)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $S_t$ is the exponentially weighted moving average of past squared gradients,\n",
        "- $\\gamma$ is the decay rate (typically set around 0.9),\n",
        "- $dW_t$ represents the current gradient,\n",
        "- $\\alpha$ is the learning rate,\n",
        "- $\\epsilon$ is a small constant added for numerical stability.\n",
        "\n",
        "These equations show that RMSProp normalizes the gradient updates using a moving average of squared gradients, preventing drastic parameter updates and improving training stability."
      ],
      "metadata": {
        "id": "q1l37G_T8gOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(W, dW, lr, S, beta, epsilon):\n",
        "  \"\"\"\n",
        "  Performs a single step of RMSProp\n",
        "  Argument:\n",
        "    W = np.array([w1, w2])\n",
        "    dW = np.array([df_dw1, df_dw2])\n",
        "    lr = learning rate\n",
        "    S = np.array([S_w1, S_w2])\n",
        "    beta = momentum coefficient\n",
        "    epsilon = small constant\n",
        "  Returns:\n",
        "    W = np.array([w1, w2])\n",
        "    S = np.array([S_w1, S_w2])\n",
        "  \"\"\"\n",
        "  S = beta*S + (1-beta)*dW**2\n",
        "  W = W - lr*dW/np.sqrt(S + epsilon)\n",
        "  return W, S"
      ],
      "metadata": {
        "id": "O_CK1wU48fXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rmsprop(w1, w2, lr, epochs, beta, epsilon = 1e-6):\n",
        "  #Init input\n",
        "  W = np.array([w1, w2], dtype = np.float32)\n",
        "  results = [W]\n",
        "  S = np.array([0, 0], dtype = np.float32)\n",
        "  #Run\n",
        "  for epoch in range(epochs):\n",
        "    dW = df_w(W)\n",
        "    W, S = rmsprop(W, dW, lr, S, beta, epsilon)\n",
        "    results.append(W)\n",
        "  return results"
      ],
      "metadata": {
        "id": "MuEz1ABJ87Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adam Optimization\n",
        "\n",
        "Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the advantages of both Momentum and RMSProp. It maintains estimates of both the first moment (mean) and the second moment (uncentered variance) of the gradients, leading to adaptive learning rates for different parameters.\n",
        "\n",
        "The update equations for Adam are:\n",
        "\n",
        "$$\n",
        "V_t = \\beta_1 V_{t-1} + (1 - \\beta_1) dW_t \\quad (4.1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "S_t = \\beta_2 S_{t-1} + (1 - \\beta_2) dW_t^2 \\quad (4.2)\n",
        "$$\n",
        "\n",
        "Bias correction terms:\n",
        "\n",
        "$$\n",
        "V_{corr} = \\frac{V_t}{1 - \\beta_1^t} \\quad (4.3)\n",
        "$$\n",
        "\n",
        "$$\n",
        "S_{corr} = \\frac{S_t}{1 - \\beta_2^t} \\quad (4.4)\n",
        "$$\n",
        "\n",
        "Final parameter update:\n",
        "\n",
        "$$\n",
        "W_t = W_t - \\alpha \\cdot \\frac{V_{corr}}{\\sqrt{S_{corr}} + \\epsilon} \\quad (4.5)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $V_t$ is the exponentially weighted moving average of past gradients (first moment),\n",
        "- $S_t$ is the exponentially weighted moving average of squared gradients (second moment),\n",
        "- $\\beta_1$ and $\\beta_2$ are decay rates (typically $\\beta_1 = 0.9$, $\\beta_2 = 0.999$),\n",
        "- $\\alpha$ is the learning rate,\n",
        "- $\\epsilon$ is a small constant for numerical stability.\n",
        "\n",
        "Adam adjusts the learning rate for each parameter dynamically, making it effective for training deep networks with noisy gradients and sparse data.\n"
      ],
      "metadata": {
        "id": "QChSp0NL9alq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(W, dw, lr, V, S, t, beta1, beta2, epsilon):\n",
        "  \"\"\"\n",
        "  Performs a single step of Adam\n",
        "  Argument:\n",
        "    W = np.array([w1, w2])\n",
        "    dw = np.array([df_dw1, df_dw2])\n",
        "    lr = learning rate\n",
        "    V = np.array([V_w1, V_w2])\n",
        "    S = np.array([S_w1, S_w2])\n",
        "    t = iteration\n",
        "    beta1 = momentum coefficient\n",
        "    beta2 = momentum coefficient\n",
        "    epsilon = small constant\n",
        "  Returns:\n",
        "    W = np.array([w1, w2])\n",
        "    V = np.array([V_w1, V_w2])\n",
        "    S = np.array([S_w1, S_w2])\n",
        "  \"\"\"\n",
        "  V = beta1*V + (1-beta1)*dw\n",
        "  S = beta2*S + (1-beta2)*dw**2\n",
        "  V_corr = V/(1-beta1**t)\n",
        "  S_corr = S/(1-beta2**t)\n",
        "  W = W - lr*V_corr/np.sqrt(S_corr + epsilon)\n",
        "  return W, V, S"
      ],
      "metadata": {
        "id": "ox6hjAjh9fz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_adam(w1, w2, lr, epochs, beta1, beta2, epsilon):\n",
        "  #Init input\n",
        "  W = np.array([w1, w2], dtype = np.float32)\n",
        "  results = [W]\n",
        "  V = np.array([0, 0], dtype = np.float32)\n",
        "  S = np.array([0, 0], dtype = np.float32)\n",
        "  #Run\n",
        "  for epoch in range(epochs):\n",
        "    dW = df_w(W)\n",
        "    W, V, S = adam(W, dW, lr, V, S, epoch+1, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)\n",
        "    results.append(W)\n",
        "  return results"
      ],
      "metadata": {
        "id": "_qmgOT4D-APi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "Fz8ko9Cg-fBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_sgd(w1 = -5, w2= -2, lr = 0.4, epochs = 30)\n",
        "print(results[2])\n",
        "print(results[-1])\n",
        "f = lambda x: 0.1*x[0]**2 + 2*x[1]**2\n",
        "print(f(results[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlGql6O6-gJZ",
        "outputId": "130bb424-b048-4e66-a86d-e44e5ae30f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.232 -0.72 ]\n",
            "[-4.09831018e-01 -4.42147839e-07]\n",
            "0.0167961463225951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_momentum(w1 = -5, w2 = -2, lr = 0.6, epochs = 30, beta = 0.5)\n",
        "print(results[2])\n",
        "print(results[-1])\n",
        "f = lambda x: 0.1*x[0]**2 + 2*x[1]**2\n",
        "print(f(results[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KyLJpIB__Cw",
        "outputId": "049b279e-b496-4828-b77a-f0728124a181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.268  1.12 ]\n",
            "[-6.10072592e-02  6.45162933e-05]\n",
            "0.00037219689264946426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_rmsprop(w1 = -5, w2 = -2,\n",
        "                        lr = 0.3, epochs = 30,\n",
        "                        beta = 0.9, epsilon = 1e-6)\n",
        "print(results[2])\n",
        "print(results[-1])\n",
        "f = lambda x: 0.1*x[0]**2 + 2*x[1]**2\n",
        "print(f(results[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQHmgrFRA3yE",
        "outputId": "2e80c111-9b70-4e90-814d-4442f948702d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.43519754 -0.59152343]\n",
            "[-3.00577081e-03 -3.00506084e-17]\n",
            "9.034658153058885e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_adam(w1 = -5, w2 = -2,\n",
        "                     lr = 0.2, epochs = 30,\n",
        "                     beta1 = 0.9, beta2 = 0.999,\n",
        "                     epsilon = 1e-6)\n",
        "\n",
        "print(results[2])\n",
        "print(results[-1])\n",
        "f = lambda x: 0.1*x[0]**2 + 2*x[1]**2\n",
        "print(f(results[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beRbhInVBV1G",
        "outputId": "e613f7da-08ae-4a77-b0bb-3394db10cdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.60025438 -1.60082446]\n",
            "[-0.11386029  0.06793503]\n",
            "0.010526754073285478\n"
          ]
        }
      ]
    }
  ]
}