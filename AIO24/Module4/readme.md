# Module 4

## **Overview**
Module 4 of the **AIO24** course explores key optimization techniques commonly used in training machine learning models. This module focuses on both classical optimization algorithms and evolutionary strategies that help fine-tune models for improved performance.

## **Topics Covered**

### **1. Linear Regression**
- **Fundamental Supervised Learning Technique** used for predicting a continuous output based on input features.
- **Key Concepts**:
  - Hypothesis representation: \( h_\theta(x) = \theta_0 + \theta_1 x \)
  - Cost function: Mean Squared Error (MSE)
  - Analytical solution using **Normal Equation**.

### **2. Gradient Descent**
- **Optimization Algorithm** used to minimize the cost function of a model.
- **Types**:
  - **Batch Gradient Descent**
  - **Stochastic Gradient Descent (SGD)**
  - **Mini-batch Gradient Descent**
- **Concepts Covered**:
  - Learning rate tuning
  - Convergence behavior
  - Application to linear regression

### **3. Genetic Algorithm**
- **Evolutionary Algorithm** inspired by natural selection.
- **How It Works**:
  - Population initialization, fitness evaluation
  - Selection, crossover, and mutation operations
  - Iterative improvement over generations
- **Applications**:
  - Optimization problems
  - Feature selection
  - Non-convex objective functions
